{"_default": {"1": {"dateRead": "2019-06-25", "source": "online", "name": "test paper", "type": "paper", "comments": "# Test header\r\nsomething under the header\r\n\r\n1. a numbered\r\n1. list for\r\n1. markdown checking\r\n\r\n- and a bunch of\r\n- regular\r\n- list items\r\n- down here\r\n\r\n### Smaller header\r\nyup\r\n\r\n# and a big header\r\nim done now", "datePublished": "1995-08-19"}, "2": {"dateRead": "2019-05-31", "source": "QData", "name": "Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers", "type": "paper", "comments": "- Categorizing NLPAE techniques: (1) targeted/untargeted? (2) choice of \u0394 (3) black-box or white-box?\r\n- Unlike images, text data is discrete, so it is hard to define \u0394x (main issue: is 'a' closer to 'b' than 'c'? no. but #000 is closer to #111 than #fff)\r\n- I did not know that CNNs worked so well on text data!\r\n\r\n### Adversarial Sequence Generation:\r\n(1) Score characters to figure out which are most important\r\n    - four scoring methods: Replace-1, Temporal Head, Temporal Tail, Temporal Head+Tail\r\n(2) transform into slightly different characters to form a new sequence that fools the classifier; basically just replace words with 'unknown'\r\n     - four techniques for this: swap two letters, substitute letter, delete letter, insert letter\r\n\r\n- DeepWordBug works well; is black-box; and it is fast", "datePublished": "2018-05-23"}, "3": {"dateRead": "2019-09-30", "source": "ICLR 2019", "title": "On The Power of Curriculum Learning in Training Deep Networks", "type": "paper", "comments": "##### Summary\r\nThis paper used curriculum learning to optimize training in convolutional neural networks. Their CNNs with curriculum learning converged faster and to better solutions than non-curriculum alternatives. \r\n\r\nThey define two important metrics:\r\n(1) The scoring function, which measures how \"\"difficult\"\" a sample is\r\n(2) The pacing function, which measures how quickly new material should be introduced\r\n\r\nThey discuss various scoring and pacing functions. Scoring is just the confidence score of a model trained previously on this data (they compare Inception network and a vanilla version of their model.)\r\n\r\nPacing can be done in different ways, they compare fixed exponential, varied exponential, and single step. It seems like single step worked best.\r\n\r\n##### Future Work\r\nI think this would be possible for text. It would be easy to use Flesch-kincaid as a scoring function. We should try that and compare it with a classifier-based scoring function. We could also probably come up with a better statistical alternative to Flesch-Kincaid that tries to avoid teaching the model new words at first.\r\n\r\nIf this worked, it would allow for the creation of a low-overhead DataLoader extension in Python that trains *everyone's* language models faster, with only a little bit of code!\r\n\r\n##### Other Thoughts\r\nThey note that \"most of the power of CL lies at the beginning of training.\" This is a good quote. It makes sense that their would be exponential decay in the impact of curriculum learning.", "datePublished": "2019-05-29"}, "4": {"dateRead": "2019-06-25", "source": "ppapwfijpiwfj", "title": "ANOTHER SAMPLE PPAPEPR", "type": "paper", "comments": "qqwhoh!?", "datePublished": "2011-08-19"}, "5": {"dateRead": "2019-06-25", "source": "qijqe", "title": "fourth try", "type": "paper", "comments": "seventteeen \r\n#thirty eight", "datePublished": "2011-08-19"}}}